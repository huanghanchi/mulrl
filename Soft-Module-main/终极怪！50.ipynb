{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/root/Downloads/metaworld-master/metaworld-master/')\n",
    "import metaworld\n",
    "import random\n",
    "\n",
    "ml10 = metaworld.MT50() # Construct the benchmark, sampling tasks\n",
    "\n",
    "training_envs = []\n",
    "for name, env_cls in ml10.train_classes.items():\n",
    "  env = env_cls()\n",
    "  task = random.choice([task for task in ml10.train_tasks\n",
    "                        if task.env_name == name])\n",
    "  env.set_task(task)\n",
    "  training_envs.append(env)\n",
    "\n",
    "for env in training_envs:\n",
    "  obs = env.reset()  # Reset environment\n",
    "  a = env.action_space.sample()  # Sample an action\n",
    "  obs, reward, done, info = env.step(a)  # Step the environoment with the sampled random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs=training_envs\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./\")\n",
    "import metaworld\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import time\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torchrl.utils import get_args\n",
    "from torchrl.utils import get_params\n",
    "from torchrl.env import get_env\n",
    "\n",
    "\n",
    "from torchrl.utils import Logger\n",
    "import torchrl.policies as policies\n",
    "import torchrl.networks as networks\n",
    "from torchrl.collector.base import BaseCollector\n",
    "from torchrl.algo import SAC\n",
    "from torchrl.algo import TwinSAC\n",
    "from torchrl.algo import TwinSACQ\n",
    "from torchrl.algo import MTSAC\n",
    "from torchrl.collector.para import ParallelCollector\n",
    "from torchrl.collector.para import AsyncParallelCollector\n",
    "from torchrl.collector.para.mt import SingleTaskParallelCollectorBase\n",
    "from torchrl.replay_buffers import BaseReplayBuffer\n",
    "from torchrl.replay_buffers.shared import SharedBaseReplayBuffer\n",
    "from torchrl.replay_buffers.shared import AsyncSharedReplayBuffer\n",
    "import gym\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Wrapper\n",
    "from gym.spaces import Box\n",
    "import numpy as np\n",
    "from metaworld.envs.mujoco.sawyer_xyz import *\n",
    "from metaworld.core.serializable import Serializable\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from torchrl.env.continuous_wrapper import *\n",
    "from torchrl.env.get_env import wrap_continuous_env\n",
    "\n",
    "\n",
    "class SingleWrapper(Wrapper):\n",
    "    def __init__(self, env):\n",
    "        self._env = env\n",
    "        self.action_space = env.action_space\n",
    "        self.observation_space = env.observation_space\n",
    "        self.train_mode = True\n",
    "    def reset(self):\n",
    "        return self._env.reset()\n",
    "\n",
    "    def seed(self, se):\n",
    "        self._env.seed(se)\n",
    "\n",
    "    def reset_with_index(self, task_idx):\n",
    "        return self._env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self._env.step(action)\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def train(self):\n",
    "        self.train_mode = True\n",
    "\n",
    "    def test(self):\n",
    "        self.train_mode = False\n",
    "    def eval(self):\n",
    "        self.train_mode = False\n",
    "\n",
    "    def render(self, mode='human', **kwargs):\n",
    "        return self._env.render(mode=mode, **kwargs)\n",
    "\n",
    "    def close(self):\n",
    "        self._env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer():\n",
    "    def __init__(self, shape, clip=10.):\n",
    "        self.shape = shape\n",
    "        self._mean = np.zeros(shape)\n",
    "        self._var = np.ones(shape)\n",
    "        self._count = 1e-4\n",
    "        self.clip = clip\n",
    "        self.should_estimate = True\n",
    "\n",
    "    def stop_update_estimate(self):\n",
    "        self.should_estimate = False\n",
    "\n",
    "    def update_estimate(self, data):\n",
    "        if not self.should_estimate:\n",
    "            return\n",
    "        if len(data.shape) == self.shape:\n",
    "            data = data[np.newaxis, :]\n",
    "        self._mean, self._var, self._count = update_mean_var_count(\n",
    "            self._mean, self._var, self._count,\n",
    "            np.mean(data, axis=0), np.var(data, axis=0), data.shape[0])\n",
    "\n",
    "    def inverse(self, raw):\n",
    "        return raw * np.sqrt(self._var) + self._mean\n",
    "\n",
    "    def inverse_torch(self, raw):\n",
    "        return raw * torch.Tensor(np.sqrt(self._var)).to(raw.device) \\\n",
    "            + torch.Tensor(self._mean).to(raw.device)\n",
    "\n",
    "    def filt(self, raw):\n",
    "        return np.clip(\n",
    "            (raw - self._mean) / (np.sqrt(self._var) + 1e-4),\n",
    "            -self.clip, self.clip)\n",
    "\n",
    "    def filt_torch(self, raw):\n",
    "        return torch.clamp(\n",
    "            (raw - torch.Tensor(self._mean).to(raw.device)) / \\\n",
    "            (torch.Tensor(np.sqrt(self._var) + 1e-4).to(raw.device)),\n",
    "            -self.clip, self.clip)\n",
    "\n",
    "class parser:\n",
    "    def __init__(self): \n",
    "        self.config='config/sac_ant.json'\n",
    "        self.id='sac_ant'\n",
    "        self.worker_nums=10\n",
    "        self.eval_worker_nums=10\n",
    "        self.seed=20\n",
    "        self.vec_env_nums=1\n",
    "        self.save_dir='./save/sac_ant'\n",
    "        self.log_dir='./log/sac_ant'\n",
    "        self.no_cuda=True\n",
    "        self.overwrite=True\n",
    "        self.device='cpu'\n",
    "        self.cuda=False\n",
    "                \n",
    "args=parser()\n",
    "params = get_params(args.config)\n",
    "\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda:{}\".format(args.device) if args.cuda else \"cpu\")\n",
    "\n",
    "normalizer=Normalizer(env.observation_space.shape)\n",
    "env.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "buffer_param = params['replay_buffer']\n",
    "\n",
    "experiment_name = os.path.split(\n",
    "    os.path.splitext(args.config)[0])[-1] if args.id is None \\\n",
    "    else args.id\n",
    "logger = Logger(\n",
    "    experiment_name, params['env_name'], args.seed, params, args.log_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import torchrl.algo.utils as atu\n",
    "\n",
    "import gym\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "class RLAlgo():\n",
    "    \"\"\"\n",
    "    Base RL Algorithm Framework\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        env = None,\n",
    "        replay_buffer = None,\n",
    "        collector = None,\n",
    "        logger = None,\n",
    "        continuous = None,\n",
    "        discount=0.99,\n",
    "        num_epochs = 3000,\n",
    "        epoch_frames = 1000,\n",
    "        max_episode_frames = 999,\n",
    "        batch_size = 128,\n",
    "        device = 'cpu',\n",
    "        train_render = False,\n",
    "        eval_episodes = 1,\n",
    "        eval_render = False,\n",
    "        save_interval = 100,\n",
    "        save_dir = None\n",
    "    ):\n",
    "\n",
    "        self.env = env\n",
    "        self.total_frames = 0\n",
    "        self.continuous = isinstance(self.env.action_space, gym.spaces.Box)\n",
    "\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.collector = collector        \n",
    "        # device specification\n",
    "        self.device = device\n",
    "\n",
    "        # environment relevant information\n",
    "        self.discount = discount\n",
    "        self.num_epochs = num_epochs\n",
    "        self.epoch_frames = epoch_frames\n",
    "        self.max_episode_frames = max_episode_frames\n",
    "\n",
    "        self.train_render = train_render\n",
    "        self.eval_render = eval_render\n",
    "\n",
    "        # training information\n",
    "        self.batch_size = batch_size\n",
    "        self.training_update_num = 0\n",
    "        self.sample_key = None\n",
    "\n",
    "        # Logger & relevant setting\n",
    "        self.logger = logger\n",
    "\n",
    "        \n",
    "        self.episode_rewards = deque(maxlen=30)\n",
    "        self.training_episode_rewards = deque(maxlen=30)\n",
    "        self.eval_episodes = eval_episodes\n",
    "\n",
    "        self.save_interval = save_interval\n",
    "        self.save_dir = save_dir\n",
    "        if not osp.exists( self.save_dir ):\n",
    "            os.mkdir( self.save_dir )\n",
    "\n",
    "        self.best_eval = None\n",
    "\n",
    "    def start_epoch(self):\n",
    "        pass\n",
    "\n",
    "    def finish_epoch(self):\n",
    "        return {}\n",
    "\n",
    "    def pretrain(self):\n",
    "        pass\n",
    "    \n",
    "    def update_per_epoch(self):\n",
    "        pass\n",
    "\n",
    "    def snapshot(self, prefix, epoch):\n",
    "        for name, network in self.snapshot_networks:\n",
    "            model_file_name=\"model_{}_{}.pth\".format(name, epoch)\n",
    "            model_path=osp.join(prefix, model_file_name)\n",
    "            torch.save(network.state_dict(), model_path)\n",
    "\n",
    "    def train(self,epoch):\n",
    "        if epoch==1:\n",
    "            self.pf.load_state_dict(torch.load('/root/metaworld-master/newsoftmodule_24/model50_'+str(index)+'/model_pf_best.pth'))\n",
    "            self.qf.load_state_dict(torch.load('/root/metaworld-master/newsoftmodule_24/model50_'+str(index)+'/model_qf_best.pth'))\n",
    "            self.vf.load_state_dict(torch.load('/root/metaworld-master/newsoftmodule_24/model50_'+str(index)+'/model_vf_best.pth'))\n",
    "    \n",
    "            self.pretrain()\n",
    "            self.total_frames = 0\n",
    "            if hasattr(self, \"pretrain_frames\"):\n",
    "                self.total_frames = self.pretrain_frames\n",
    "\n",
    "            self.start_epoch()\n",
    "\n",
    "        self.current_epoch = epoch\n",
    "        start = time.time()\n",
    "\n",
    "        self.start_epoch()\n",
    "\n",
    "        explore_start_time = time.time()\n",
    "        training_epoch_info =  self.collector.train_one_epoch()\n",
    "        for reward in training_epoch_info[\"train_rewards\"]:\n",
    "            self.training_episode_rewards.append(reward)\n",
    "        explore_time = time.time() - explore_start_time\n",
    "\n",
    "        train_start_time = time.time()\n",
    "        loss=self.update_per_epoch()\n",
    "        train_time = time.time() - train_start_time\n",
    "\n",
    "        finish_epoch_info = self.finish_epoch()\n",
    "\n",
    "        eval_start_time = time.time()\n",
    "        eval_infos = self.collector.eval_one_epoch()\n",
    "        eval_time = time.time() - eval_start_time\n",
    "\n",
    "        self.total_frames += self.collector.active_worker_nums * self.epoch_frames\n",
    "\n",
    "        infos = {}\n",
    "\n",
    "        for reward in eval_infos[\"eval_rewards\"]:\n",
    "            self.episode_rewards.append(reward)\n",
    "        # del eval_infos[\"eval_rewards\"]\n",
    "\n",
    "        if self.best_eval is None or \\\n",
    "            np.mean(eval_infos[\"eval_rewards\"]) > self.best_eval:\n",
    "            self.best_eval = np.mean(eval_infos[\"eval_rewards\"])\n",
    "            self.snapshot(self.save_dir, 'best')\n",
    "        del eval_infos[\"eval_rewards\"]\n",
    "        infos[\"eval_avg_success_rate\"] =eval_infos[\"success\"]\n",
    "        infos[\"Running_Average_Rewards\"] = np.mean(self.episode_rewards)\n",
    "        infos[\"Running_success_rate\"] =training_epoch_info[\"train_success_rate\"]\n",
    "        infos[\"Train_Epoch_Reward\"] = training_epoch_info[\"train_epoch_reward\"]\n",
    "        infos[\"Running_Training_Average_Rewards\"] = np.mean(\n",
    "            self.training_episode_rewards)\n",
    "        infos[\"Explore_Time\"] = explore_time\n",
    "        infos[\"Train___Time\"] = train_time\n",
    "        infos[\"Eval____Time\"] = eval_time\n",
    "        infos.update(eval_infos)\n",
    "        infos.update(finish_epoch_info)\n",
    "\n",
    "        self.logger.add_epoch_info(epoch, self.total_frames,\n",
    "            time.time() - start, infos )\n",
    "\n",
    "        if epoch % self.save_interval == 0:\n",
    "            self.snapshot(self.save_dir, epoch)\n",
    "        if epoch==self.num_epochs-1:\n",
    "            self.snapshot(self.save_dir, \"finish\")\n",
    "            self.collector.terminate()\n",
    "        return loss\n",
    "    def update(self, batch):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _update_target_networks(self):\n",
    "        if self.use_soft_update:\n",
    "            for net, target_net in self.target_networks:\n",
    "                atu.soft_update_from_to(net, target_net, self.tau)\n",
    "        else:\n",
    "            if self.training_update_num % self.target_hard_update_period == 0:\n",
    "                for net, target_net in self.target_networks:\n",
    "                    atu.copy_model_params_from_to(net, target_net)\n",
    "\n",
    "    @property\n",
    "    def networks(self):\n",
    "        return [\n",
    "        ]\n",
    "    \n",
    "    @property\n",
    "    def snapshot_networks(self):\n",
    "        return [\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def target_networks(self):\n",
    "        return [\n",
    "        ]\n",
    "    \n",
    "    def to(self, device):\n",
    "        for net in self.networks:\n",
    "            net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class OffRLAlgo(RLAlgo):\n",
    "    \"\"\"\n",
    "    Base RL Algorithm Framework\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "\n",
    "        pretrain_epochs=0,\n",
    "\n",
    "        min_pool = 0,\n",
    "\n",
    "        target_hard_update_period = 1000,\n",
    "        use_soft_update = True,\n",
    "        tau = 0.001,\n",
    "        opt_times = 1,\n",
    "\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(OffRLAlgo, self).__init__(**kwargs)\n",
    "\n",
    "        # environment relevant information\n",
    "        self.pretrain_epochs = pretrain_epochs\n",
    "        \n",
    "        # target_network update information\n",
    "        self.target_hard_update_period = target_hard_update_period\n",
    "        self.use_soft_update = use_soft_update\n",
    "        self.tau = tau\n",
    "\n",
    "        # training information\n",
    "        self.opt_times = opt_times\n",
    "        self.min_pool = min_pool\n",
    "\n",
    "        self.sample_key = [ \"obs\", \"next_obs\", \"acts\", \"rewards\", \"terminals\" ]\n",
    "\n",
    "    def update_per_timestep(self):\n",
    "        if self.replay_buffer.num_steps_can_sample() > max( self.min_pool, self.batch_size ):\n",
    "            for _ in range( self.opt_times ):\n",
    "                batch = self.replay_buffer.random_batch(self.batch_size, self.sample_key)\n",
    "                infos = self.update( batch )\n",
    "                self.logger.add_update_info( infos )\n",
    "\n",
    "    def update_per_epoch(self):\n",
    "        loss=[]\n",
    "        for _ in range( self.opt_times ):\n",
    "            batch = self.replay_buffer.random_batch(self.batch_size, self.sample_key)\n",
    "            infos = self.update( batch )\n",
    "            loss.append(infos['Training/policy_loss'])\n",
    "            self.logger.add_update_info( infos )\n",
    "        return np.mean(loss)\n",
    "    def pretrain(self):\n",
    "        total_frames = 0\n",
    "        self.pretrain_epochs * self.collector.worker_nums * self.epoch_frames\n",
    "        \n",
    "        for pretrain_epoch in range( self.pretrain_epochs ):\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            self.start_epoch()\n",
    "            \n",
    "            training_epoch_info =  self.collector.train_one_epoch()\n",
    "            for reward in training_epoch_info[\"train_rewards\"]:\n",
    "                self.training_episode_rewards.append(reward)\n",
    "\n",
    "            finish_epoch_info = self.finish_epoch()\n",
    "\n",
    "            total_frames += self.collector.active_worker_nums * self.epoch_frames\n",
    "            \n",
    "            infos = {}\n",
    "            \n",
    "            infos[\"Train_Epoch_Reward\"] = training_epoch_info[\"train_epoch_reward\"]\n",
    "            infos[\"Running_Training_Average_Rewards\"] = np.mean(self.training_episode_rewards)\n",
    "            infos.update(finish_epoch_info)\n",
    "            \n",
    "            self.logger.add_epoch_info(pretrain_epoch, total_frames, time.time() - start, infos, csv_write=False )\n",
    "        \n",
    "        self.pretrain_frames = total_frames\n",
    "\n",
    "        self.logger.log(\"Finished Pretrain\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn as nn\n",
    "\n",
    "\n",
    "class SAC(OffRLAlgo):\n",
    "    \"\"\"\n",
    "    SAC\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            pf, vf, qf,\n",
    "            plr,vlr,qlr,\n",
    "            optimizer_class=optim.Adam,\n",
    "            \n",
    "            policy_std_reg_weight=1e-3,\n",
    "            policy_mean_reg_weight=1e-3,\n",
    "\n",
    "            reparameterization = True,\n",
    "            automatic_entropy_tuning = True,\n",
    "            target_entropy = None,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super(SAC, self).__init__(**kwargs)\n",
    "        self.pf = pf\n",
    "        self.qf = qf\n",
    "        self.vf = vf\n",
    "        self.target_vf = copy.deepcopy(vf)\n",
    "        self.to(self.device)\n",
    "\n",
    "        self.plr = plr\n",
    "        self.vlr = vlr\n",
    "        self.qlr = qlr\n",
    "\n",
    "        self.qf_optimizer = optimizer_class(\n",
    "            self.qf.parameters(),\n",
    "            lr=self.qlr,\n",
    "        )\n",
    "\n",
    "        self.vf_optimizer = optimizer_class(\n",
    "            self.vf.parameters(),\n",
    "            lr=self.vlr,\n",
    "        )\n",
    "\n",
    "        self.pf_optimizer = optimizer_class(\n",
    "            self.pf.parameters(),\n",
    "            lr=self.plr,\n",
    "        )\n",
    "        \n",
    "        self.automatic_entropy_tuning = automatic_entropy_tuning\n",
    "        if self.automatic_entropy_tuning:\n",
    "            if target_entropy:\n",
    "                self.target_entropy = target_entropy\n",
    "            else:\n",
    "                self.target_entropy = -np.prod(self.env.action_space.shape).item()  # from rlkit\n",
    "            self.log_alpha = torch.zeros(1).to(self.device)\n",
    "            self.log_alpha.requires_grad_()\n",
    "            self.alpha_optimizer = optimizer_class(\n",
    "                [self.log_alpha],\n",
    "                lr=self.plr,\n",
    "            )\n",
    "\n",
    "        self.qf_criterion = nn.MSELoss()\n",
    "        self.vf_criterion = nn.MSELoss()\n",
    "\n",
    "        self.policy_std_reg_weight = policy_std_reg_weight\n",
    "        self.policy_mean_reg_weight = policy_mean_reg_weight\n",
    "\n",
    "        self.reparameterization = reparameterization\n",
    "\n",
    "    def update(self, batch):\n",
    "        self.training_update_num += 1\n",
    "        \n",
    "        obs = batch['obs']\n",
    "        actions = batch['acts']\n",
    "        next_obs = batch['next_obs']\n",
    "        rewards = batch['rewards']\n",
    "        terminals = batch['terminals']\n",
    "\n",
    "        rewards = torch.Tensor(rewards).to( self.device )\n",
    "        terminals = torch.Tensor(terminals).to( self.device )\n",
    "        obs = torch.Tensor(obs).to( self.device )\n",
    "        actions = torch.Tensor(actions).to( self.device )\n",
    "        next_obs = torch.Tensor(next_obs).to( self.device )\n",
    "\n",
    "        \"\"\"\n",
    "        Policy operations.\n",
    "        \"\"\"\n",
    "        sample_info = self.pf.explore(obs, return_log_probs=True )\n",
    "\n",
    "        mean        = sample_info[\"mean\"]\n",
    "        log_std     = sample_info[\"log_std\"]\n",
    "        new_actions = sample_info[\"action\"]\n",
    "        log_probs   = sample_info[\"log_prob\"]\n",
    "        ent         = sample_info[\"ent\"]\n",
    "\n",
    "        q_pred = self.qf([obs, actions])\n",
    "        v_pred = self.vf(obs)\n",
    "\n",
    "        if self.automatic_entropy_tuning:\n",
    "            \"\"\"\n",
    "            Alpha Loss\n",
    "            \"\"\"\n",
    "            alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()\n",
    "            self.alpha_optimizer.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optimizer.step()\n",
    "            alpha = self.log_alpha.exp()\n",
    "        else:\n",
    "            alpha = 1\n",
    "            alpha_loss = 0\n",
    "\n",
    "        \"\"\"\n",
    "        QF Loss\n",
    "        \"\"\"\n",
    "        target_v_values = self.target_vf(next_obs)\n",
    "        q_target = rewards + (1. - terminals) * self.discount * target_v_values\n",
    "        qf_loss = self.qf_criterion( q_pred, q_target.detach())\n",
    "\n",
    "        \"\"\"\n",
    "        VF Loss\n",
    "        \"\"\"\n",
    "        q_new_actions = self.qf([obs, new_actions])\n",
    "        v_target = q_new_actions - alpha * log_probs\n",
    "        vf_loss = self.vf_criterion( v_pred, v_target.detach())\n",
    "\n",
    "        \"\"\"\n",
    "        Policy Loss\n",
    "        \"\"\"\n",
    "        if not self.reparameterization:\n",
    "            log_policy_target = q_new_actions - v_pred\n",
    "            policy_loss = (\n",
    "                log_probs * ( alpha * log_probs - log_policy_target).detach()\n",
    "            ).mean()\n",
    "        else:\n",
    "            policy_loss = ( alpha * log_probs - q_new_actions).mean()\n",
    "\n",
    "        std_reg_loss = self.policy_std_reg_weight * (log_std**2).mean()\n",
    "        mean_reg_loss = self.policy_mean_reg_weight * (mean**2).mean()\n",
    "\n",
    "        policy_loss += std_reg_loss + mean_reg_loss\n",
    "        \n",
    "        \"\"\"\n",
    "        Update Networks\n",
    "        \"\"\"\n",
    "        self.pf_optimizer.zero_grad()\n",
    "        \n",
    "        w=[]\n",
    "        for key in pfs[0].state_dict().keys():\n",
    "            w.append(torch.cat([pfs[j].state_dict()[key].unsqueeze(0) for j in range(len(envs))]))            \n",
    "        \n",
    "        rloss[index] = policy_loss.clone()\n",
    "        rlosscopy=rloss.copy()\n",
    "        rlosscopy[index] =rlosscopy[index].detach().item()\n",
    "        low=np.array(rlosscopy).mean()-3*np.array(rlosscopy).std()\n",
    "        high=np.array(rlosscopy).mean()+3*np.array(rlosscopy).std()\n",
    "        if np.random.random()<sum(np.array(rlosscopy)<low)+sum(np.array(rlosscopy)>high)>len(envs)/len(envs)+np.exp(-i_episode/1000)+np.exp(-np.array(rlosscopy).mean()*40):\n",
    "            pre=rloss[index]+lossw(currindex,index,rloss,w,B)/10   \n",
    "        else:\n",
    "            pre=rloss[index]\n",
    "        # compute gradients\n",
    "\n",
    "        pre.backward()\n",
    "\n",
    "        # train the NN\n",
    "    \n",
    "        self.pf_optimizer.step()\n",
    "        rloss[index]=rloss[index].detach().item()\n",
    "        \n",
    "        self.qf_optimizer.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        self.qf_optimizer.step()\n",
    "\n",
    "        self.vf_optimizer.zero_grad()\n",
    "        vf_loss.backward()\n",
    "        self.vf_optimizer.step()\n",
    "\n",
    "        self._update_target_networks()\n",
    "\n",
    "        # Information For Logger\n",
    "        info = {}\n",
    "        info['Reward_Mean'] = rewards.mean().item()\n",
    "\n",
    "        if self.automatic_entropy_tuning:\n",
    "            info[\"Alpha\"] = alpha.item()\n",
    "            info[\"Alpha_loss\"] = alpha_loss.item()\n",
    "        info['Training/policy_loss'] = policy_loss.item()\n",
    "        info['Training/vf_loss'] = vf_loss.item()\n",
    "        info['Training/qf_loss'] = qf_loss.item()\n",
    "\n",
    "        info['log_std/mean'] = log_std.mean().item()\n",
    "        info['log_std/std'] = log_std.std().item()\n",
    "        info['log_std/max'] = log_std.max().item()\n",
    "        info['log_std/min'] = log_std.min().item()\n",
    "\n",
    "        info['log_probs/mean'] = log_std.mean().item()\n",
    "        info['log_probs/std'] = log_std.std().item()\n",
    "        info['log_probs/max'] = log_std.max().item()\n",
    "        info['log_probs/min'] = log_std.min().item()\n",
    "\n",
    "        info['mean/mean'] = mean.mean().item()\n",
    "        info['mean/std'] = mean.std().item()\n",
    "        info['mean/max'] = mean.max().item()\n",
    "        info['mean/min'] = mean.min().item()\n",
    "\n",
    "        return info\n",
    "\n",
    "    @property\n",
    "    def networks(self):\n",
    "        return [\n",
    "            self.pf,\n",
    "            self.qf,\n",
    "            self.vf,\n",
    "            self.target_vf\n",
    "        ]\n",
    "    \n",
    "    @property\n",
    "    def snapshot_networks(self):\n",
    "        return [\n",
    "            [\"pf\", self.pf],\n",
    "            [\"qf\", self.qf],\n",
    "            [\"vf\", self.vf]\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def target_networks(self):\n",
    "        return [\n",
    "            ( self.vf, self.target_vf )\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pfs=[]\n",
    "qf1s=[]\n",
    "vfs=[]\n",
    "agents=[]\n",
    "epochs=[1 for i in range(len(envs))]\n",
    "for index in range(len(envs)):\n",
    "    print(index)\n",
    "    env=SingleWrapper(envs[index])\n",
    "    params = get_params(args.config)\n",
    "    params['general_setting']['logger'] =  Logger(\n",
    "            'mt50', str(index), args.seed, params, './log/mt50_'+str(index)+'/')\n",
    "    params['env_name']=str(index)\n",
    "    params['general_setting']['env'] = env\n",
    "\n",
    "    replay_buffer = BaseReplayBuffer(\n",
    "        max_replay_buffer_size=int(buffer_param['size'])#,\n",
    "    #    time_limit_filter=buffer_param['time_limit_filter']\n",
    "    )\n",
    "    params['general_setting']['replay_buffer'] = replay_buffer\n",
    "\n",
    "\n",
    "    params['general_setting']['device'] = device\n",
    "\n",
    "    params['net']['base_type'] = networks.MLPBase\n",
    "    params['net']['activation_func'] = torch.nn.ReLU\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    pf = policies.GuassianContPolicy(\n",
    "        input_shape=env.observation_space.shape[0], \n",
    "        output_shape=2 * env.action_space.shape[0],\n",
    "        **params['net'],\n",
    "        **params['sac'])\n",
    "\n",
    "    qf1 = networks.QNet(\n",
    "        input_shape=env.observation_space.shape[0] + env.action_space.shape[0],\n",
    "        output_shape=1,\n",
    "        **params['net'])\n",
    "\n",
    "    vf = networks.Net(\n",
    "            input_shape=env.observation_space.shape,\n",
    "            output_shape=1,\n",
    "            **params['net']\n",
    "        )\n",
    "    pfs.append(pf)\n",
    "    qf1s.append(qf1)\n",
    "    vfs.append(vf)\n",
    "    params['general_setting']['collector'] = BaseCollector(\n",
    "        env=env, pf=pf,\n",
    "        replay_buffer=replay_buffer, device=device,\n",
    "        train_render=False,\n",
    "        **params[\"collector\"]\n",
    "    )\n",
    "    params['general_setting']['save_dir'] = osp.join(\n",
    "        './log/', \"model50_\"+str(index))\n",
    "    agent = SAC(\n",
    "            pf=pf,\n",
    "            qf=qf1,plr=3e-4,vlr=3e-4,qlr=3e-4,\n",
    "            vf=vf,\n",
    "            **params[\"sac\"],\n",
    "            **params[\"general_setting\"]\n",
    "        )\n",
    "    agents.append(agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(envs)):\n",
    "    agents[index].pf.load_state_dict(torch.load('/root/metaworld-master/newsoftmodule_24/model50_'+str(index)+'/model_pf_best.pth'))\n",
    "    agents[index].qf.load_state_dict(torch.load('/root/metaworld-master/newsoftmodule_24/model50_'+str(index)+'/model_qf_best.pth'))\n",
    "    agents[index].vf.load_state_dict(torch.load('/root/metaworld-master/newsoftmodule_24/model50_'+str(index)+'/model_vf_best.pth'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def pss(x,points):\n",
    "    def pss0(x,i):\n",
    "        return torch.tanh(200*torch.tensor(x-i))/2+0.5\n",
    "    return len(points)-sum([pss0(x,i) for i in points])\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(0,r'/root/Downloads/metaworld-master/metaworld-master/constopt-pytorch/')\n",
    "import constopt\n",
    "from constopt.constraints import LinfBall\n",
    "from constopt.stochastic import PGD, PGDMadry, FrankWolfe, MomentumFrankWolfe\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.utils as utils\n",
    "from scipy.stats import rankdata\n",
    "def loss(rloss,w,B,mu=0.2,lamb=[0.01,0.01,0.01]):\n",
    "    return torch.tensor([1+mu*(np.linalg.norm(B[t],ord=1)-np.linalg.norm(B[t][t],ord=1)) for t in range(len(envs))]).dot(rloss)+lamb[0]*sum([sum([sum([torch.norm(w[i][t]-sum([B.T[t][j]*w[i][j] for j in range(len(envs))]),p=2)**2]) for i in range(2)]) for t in range(len(envs))])\n",
    "def losst(currindex,t,rloss,w,B,mu=0.2,lamb=[0.01,0.01,0.01],U=[13],pi=list(range(len(envs)))):\n",
    "    new_rloss=[i for i in rloss]\n",
    "    new_rloss[t]=new_rloss[t]+1\n",
    "    rlossRank=1+len(envs)-rankdata(new_rloss, method='min')\n",
    "    points=B[t]\n",
    "    return (1+mu*sum([torch.norm(torch.tensor(B[t][i]),p=1)for i in set(list(range(len(envs))))-set([t])]))*rloss[t]+lamb[0]*sum([sum([sum([torch.norm(w[i][s]-sum([B[pi[j]][s]*w[i][pi[j]] for j in range(currindex-1)])-B[t][s]*w[i][t],p=2)**2]) for i in range(2)]) for s in U])+lamb[2]*torch.norm(torch.tensor(rlossRank)-torch.tensor([pss(torch.tensor(i-0.01),points) for i in points]))**2\n",
    "def lossb(currindex,t,rloss,w,B,mu=0.2,lamb=[0.01,0.01,0.01],U=[13],pi=list(range(len(envs)))):\n",
    "    new_rloss=[i for i in rloss]\n",
    "    new_rloss[t]=new_rloss[t]+1\n",
    "    rlossRank=1+len(envs)-rankdata(new_rloss, method='min')\n",
    "    points=B[t]\n",
    "    return (1+mu*sum([torch.norm(B[t][i],p=1)for i in set(list(range(len(envs))))-set([t])]))*rloss[t]+lamb[0]*sum([sum([sum([torch.norm(w[i][s]-sum([B[pi[j]][s]*w[i][pi[j]] for j in range(currindex-1)])-B[t][s]*w[i][t],p=2)**2]) for i in range(2)]) for s in U])+lamb[2]*torch.norm(torch.tensor(rlossRank)-torch.tensor([pss(torch.tensor(i-0.01),points) for i in points]))**2\n",
    "def lossw(currindex,t,rloss,w,B,mu=0.2,lamb=[0.01,0.01,0.01],U=[13],pi=list(range(len(envs)))):\n",
    "    new_rloss=[i for i in rloss]\n",
    "    new_rloss[t]=new_rloss[t]+1\n",
    "    rlossRank=1+len(envs)-rankdata(new_rloss, method='min')\n",
    "    points=B[t]\n",
    "    return (1+mu*sum([torch.norm(torch.tensor(B[t][i]),p=1)for i in set(list(range(len(envs))))-set([t])]))*rloss[t]+lamb[0]*sum([sum([torch.norm(w[i][t]-sum([B[pi[j]][t]*w[i][pi[j]] for j in range(currindex-1)]),p=2)**2]) for i in range(2)])+lamb[2]*torch.norm(torch.tensor(rlossRank)-torch.tensor([pss(torch.tensor(i-0.01),points) for i in points]))**2\n",
    "def lossw2(currindex,t,rloss,w,B,mu=0.2,lamb=[0.01,0.01,0.01],U=[13],pi=list(range(len(envs)))):\n",
    "    points=B[t]\n",
    "    return (1+mu*sum([torch.norm(torch.tensor(B[t][i]),p=1)for i in set(list(range(len(envs))))-set([t])]))*rloss+lamb[0]*sum([sum([torch.norm(w[i][t]-sum([B[pi[j]][t]*w[i][pi[j]] for j in range(currindex-1)]),p=2)**2]) for i in range(2)])\n",
    "#+0*torch.norm(torch.tensor(priors[current])-torch.tensor([pss(torch.tensor(i-0.01),points) for i in points]))**2\n",
    "import torch.optim as optim\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "OPTIMIZER_CLASSES = [FrankWolfe]# [PGD, PGDMadry, FrankWolfe, MomentumFrankWolfe]\n",
    "radius=0.05\n",
    "\n",
    "def setup_problem(make_nonconvex=False):\n",
    "    radius2 = radius\n",
    "    loss_func=lossb\n",
    "    constraint = LinfBall(radius2)\n",
    "\n",
    "    return loss_func, constraint\n",
    "\n",
    "\n",
    "def optimize(loss_func, constraint, optimizer_class, iterations=100):\n",
    "    for i in range(len(envs)):\n",
    "        if i!=t:\n",
    "            B[t][i] =torch.tensor(B[t][i],requires_grad=True)\n",
    "    optimizer = [optimizer_class([B[t][i]], constraint) for i in set(list(range(len(envs))))-set([t])]\n",
    "    iterates = [[B[t][i].data if i!=t else B[t][i] for i in range(len(envs))]]\n",
    "    losses = []\n",
    "    # Use Madry's heuristic for step size\n",
    "    step_size = {\n",
    "        FrankWolfe.name: None,\n",
    "        MomentumFrankWolfe.name: None,\n",
    "        PGD.name: 2.5 * constraint.alpha / iterations * 2.,\n",
    "        PGDMadry.name: 2.5 * constraint.alpha / iterations\n",
    "    }\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        for i in range(len(envs)-1):\n",
    "            optimizer[i].zero_grad()\n",
    "        loss = loss_func(currindex,t,rloss,w,B,U=list(set(U)-set(list([t]))))\n",
    "        loss.backward(retain_graph=True)\n",
    "        for i in  range(len(envs)-1):\n",
    "            optimizer[i].step(step_size[optimizer[i].name])\n",
    "        for i in set(list(range(len(envs))))-set([t]):\n",
    "            B[t][i].data.clamp_(0,100)\n",
    "        losses.append(loss)\n",
    "        iterates.append([B[t][i].data if i!=t else B[t][i] for i in range(len(envs))])\n",
    "    loss = loss_func(currindex,t,rloss,w,B,U=list(set(U)-set(list([t])))).detach()\n",
    "    losses.append(loss)\n",
    "    B[t]=[B[t][i].data if i!=t else B[t][i] for i in range(len(envs))]\n",
    "    return losses, iterates\n",
    "paras=[[0.01,0.01,0.01,0.01],\n",
    "[1,0.01,0.01,0.01],\n",
    "[1,0.01,0.01,0.02],\n",
    "[0.01,0.01,0,0.1],\n",
    "[0.01,0.01,0,0.05],\n",
    "[0.5,0.01,0.01,0.01],\n",
    "[1,0.01,0.01,0.1],\n",
    "[0.01,0.01,0,1],\n",
    "[0.01,0.01,0,0],\n",
    "[0.1,0.01,0,0.01],\n",
    "[0.2,0.01,0,0.05],\n",
    "[0.1,0.01,0,0.05],\n",
    "[0.01,0.01,0.01,0.5],\n",
    "[1,0.01,0.02,0.02],\n",
    "[1,0.01,0.02,0.01],\n",
    "[0.1,0.01,0,0.2],\n",
    "[1,0.01,0.02,0.05]]\n",
    "para=paras[5]\n",
    "mu,lamb=0.01,[0.01,0.01,0.01]#para[0],[para[1],para[2],para[3]]\n",
    "rloss=[0.0 for i in range(len(envs))]\n",
    "rewardsRec=[[] for i in range(len(envs))]\n",
    "rewardsRec_nor=[[0] for i in range(len(envs))]\n",
    "succeessRec=[[] for i in range(len(envs))]\n",
    "TotalRewardRec=[]\n",
    "B=[list(i) for i in np.diag(np.ones(len(envs)))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(10000):\n",
    "    rlosscopy=rloss.copy()\n",
    "    low=np.array(rlosscopy).mean()-3*np.array(rlosscopy).std()\n",
    "    high=np.array(rlosscopy).mean()+3*np.array(rlosscopy).std()\n",
    "    if np.random.random()<sum(np.array(rlosscopy)<low)+sum(np.array(rlosscopy)>high)>len(envs)/len(envs)+np.exp(-i_episode/1000)+np.exp(-np.array(rlosscopy).mean()*40):\n",
    "        p = np.random.random()\n",
    "        # roll = np.random.randint(2)\n",
    "        length = 0\n",
    "        w=[]\n",
    "        for key in pfs[0].state_dict().keys():\n",
    "            w.append(torch.cat([pfs[j].state_dict()[key].unsqueeze(0) for j in range(len(envs))]))            \n",
    "        \n",
    "        # if np.random.random()<sum(np.array(rloss)<low)+sum(np.array(rloss)>high)>len(envs)/len(envs)+np.exp(-rnd/1000)+np.exp(-np.array(rloss).mean()*40):\n",
    "        #    multitask=True\n",
    "        #else:\n",
    "         #   multitask=False\n",
    "        #rloss=torch.tensor([0 for i in range(len(envs))])\n",
    "        #update pi\n",
    "        U=list(range(len(envs)))\n",
    "        pi=[0 for i in range(len(envs))]\n",
    "        for currindex in range(len(envs)):\n",
    "            indexdict={}\n",
    "            for t in U:\n",
    "                indexdict[losst(currindex,t,torch.tensor(rloss),w,B,mu=mu,lamb=lamb,U=list(set(U)-set(list([t])))).item()]=t\n",
    "            t=indexdict[min(indexdict.keys())]\n",
    "            pi[currindex]=t\n",
    "            U=list(set(U)-set(list([pi[currindex]])))  \n",
    "            #update b\n",
    "            loss_func, constraint = setup_problem(make_nonconvex=True)\n",
    "            iterations = 10\n",
    "            for opt_class in OPTIMIZER_CLASSES:\n",
    "                losses_, iterates_ = optimize(loss_func,\n",
    "                                              constraint,\n",
    "                                              opt_class,\n",
    "                                              iterations)\n",
    "      #      B[t]=torch.tensor(B[t],requires_grad=True)\n",
    "       #     optimizer=torch.optim.Adam([B[t]],lr=1e-2)\n",
    "        #    for step in range(5):\n",
    "         #       pre=lossb(currindex,t,torch.tensor(rloss),w,B,U=list(set(U)-set(list([t]))))\n",
    "          #      optimizer.zero_grad()\n",
    "           #     pre.backward(retain_graph=True)\n",
    "            #    optimizer.step()\n",
    "            #B[t]=B[t].detach().numpy()\n",
    "\n",
    "\n",
    "            env_id=t\n",
    "            index=t\n",
    "            env=envs[index]\n",
    "            print('-------------------------------------------------------------------------------',i_episode,index)\n",
    "            agents[index].train(epochs[index])\n",
    "            epochs[index]+=1\n",
    "            np.save('B_oursall0.01_mt50.npy',B)\n",
    "    else:\n",
    "        \n",
    "        for index, env in enumerate(envs):\n",
    "            print('-------------------------------------------------------------------------------',i_episode,index)\n",
    "            agents[index].train(epochs[index])\n",
    "            epochs[index]+=1\n",
    "        np.save('B_oursall0.01_mt50.npy',B)\n",
    "        \n",
    "if i_episode == 5999:\n",
    "    print('Well that failed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
